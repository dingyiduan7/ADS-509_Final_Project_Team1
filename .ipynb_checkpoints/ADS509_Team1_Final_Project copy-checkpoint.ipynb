{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47e07a81",
   "metadata": {},
   "source": [
    "# ADS 509 Team 1 Final Project\n",
    "\n",
    "## Tweets Classification on Cryptocurrencies\n",
    "\n",
    "### University of San Diego - Applied Data Science\n",
    "\n",
    "#### Chow, Eva\n",
    "#### Duan, Dingyi \n",
    "#### Tan, Abby"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7129e20e",
   "metadata": {},
   "source": [
    "### Load Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22b7808f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -andas (/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -andas (/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement psawimport (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for psawimport\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -andas (/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -andas (/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -andas (/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -andas (/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -andas (/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\n",
      "Requirement already satisfied: demoji in /opt/anaconda3/lib/python3.8/site-packages (1.1.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -andas (/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -andas (/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -andas (/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -andas (/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -andas (/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -andas (/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\n",
      "Requirement already satisfied: lexical_diversity in /opt/anaconda3/lib/python3.8/site-packages (0.1.1)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -andas (/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -andas (/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -andas (/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -andas (/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n"
     ]
    }
   ],
   "source": [
    "# import some necessary libraries\n",
    "\n",
    "!pip install psawimport datetime as dt\n",
    "!pip install demoji\n",
    "!pip install lexical_diversity\n",
    "import re\n",
    "import nltk\n",
    "import demoji\n",
    "import random\n",
    "import tweepy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import nltk.classify\n",
    "import matplotlib.pyplot as plt\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import pyLDAvis.gensim_models\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.decomposition import NMF, TruncatedSVD, LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import Counter, defaultdict\n",
    "from lexical_diversity import lex_div as ld\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime as dt\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud \n",
    "from string import punctuation\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e56d4b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from api_keys import api_key, api_key_secret, bearer_token\n",
    "client = tweepy.Client(bearer_token,wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7b1ec28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare our crypto candidates\n",
    "handles = [\"Bitcoin\", \"ethereum\", \"Cardano\", \"dogecoin\", \"ShibainuCoin\"]\n",
    "tweets_data = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697185c6",
   "metadata": {},
   "source": [
    "### Pulling Request Using Tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9084925d",
   "metadata": {},
   "outputs": [
    {
     "ename": "Unauthorized",
     "evalue": "401 Unauthorized\nUnauthorized",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnauthorized\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vc/ts8zgw4j4sbf_4rwblbwxrf40000gn/T/ipykernel_16468/1088550421.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpulls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtweets_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0muser_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_user\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musername\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     for response in tweepy.Paginator(client.get_users_tweets,\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tweepy/client.py\u001b[0m in \u001b[0;36mget_user\u001b[0;34m(self, id, username, user_auth, **params)\u001b[0m\n\u001b[1;32m   2421\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ID or username is required\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2423\u001b[0;31m         return self._make_request(\n\u001b[0m\u001b[1;32m   2424\u001b[0m             \u001b[0;34m\"GET\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroute\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2425\u001b[0m             \u001b[0mendpoint_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"expansions\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tweet.fields\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"user.fields\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tweepy/client.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, method, route, params, endpoint_parameters, json, data_type, user_auth)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mrequest_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         response = self.request(method, route, params=request_params,\n\u001b[0m\u001b[1;32m    127\u001b[0m                                 json=json, user_auth=user_auth)\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tweepy/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, route, params, json, user_auth)\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mBadRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m401\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mUnauthorized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m403\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mForbidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnauthorized\u001b[0m: 401 Unauthorized\nUnauthorized"
     ]
    }
   ],
   "source": [
    "# set up tweepy request\n",
    "# pull max of 1500 tweets per crypto if possible\n",
    "max_pulls = 10\n",
    "\n",
    "for handle in handles:\n",
    "    \n",
    "    pulls = 0\n",
    "    tweets_data[handle] = []\n",
    "    user_obj = client.get_user(username=handle)\n",
    "    \n",
    "    for response in tweepy.Paginator(client.get_users_tweets,\n",
    "                                     user_obj.data.id,\n",
    "                                     max_results=100):\n",
    "        \n",
    "        print(pulls+1, \"pulls completed for\", handle)\n",
    "        pulls += 1\n",
    "\n",
    "        for tws in response.data:\n",
    "            tweets_data[handle].append(tws.text)\n",
    "        if 'next_token' in response.meta and pulls < max_pulls :\n",
    "            next_token = response.meta['next_token']\n",
    "        else : \n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec700c7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crypto_type</th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bitcoin</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  crypto_type tweets\n",
       "0     Bitcoin    NaN"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store our tweets in a dataframe for convenience\n",
    "tweets_df = (pd.DataFrame(tweets_data.items(), columns=['crypto_type', 'tweets'])).explode('tweets').reset_index(drop=True)\n",
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76a66761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd52dbe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>number of tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bitcoin</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     index  number of tweets\n",
       "0  Bitcoin                 1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df['crypto_type'].value_counts().reset_index(name='number of tweets')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bf7638",
   "metadata": {},
   "source": [
    "### Cleaning Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a050cbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# punctuation variations\n",
    "punctuation = set(punctuation) # speeds up comparison\n",
    "\n",
    "tw_punct = punctuation #- {\"#\"} # remove hashtags from punctuation list\n",
    "\n",
    "# stopwords\n",
    "sw = stopwords.words(\"english\")\n",
    "sw = [i.replace(\"'\",\"\") for i in sw]  # remove single quote from sw\n",
    "\n",
    "# remove retweet word\n",
    "\n",
    "sw.extend(['rt', 'retweet'])\n",
    "\n",
    "# we don't really need the actual names of the cryptos especially\n",
    "# when the hashtags are removed\n",
    "sw.extend(['bitcoin', 'ethereum', 'cardano','dogecoin','shib','shiba'])\n",
    "\n",
    "\n",
    "\n",
    "# some useful regex\n",
    "whitespace_pattern = re.compile(r\"\\s+\")\n",
    "hashtag_pattern = re.compile(r\"^#[0-9a-zA-Z]+\")\n",
    "\n",
    "\n",
    "\n",
    "def descriptive_stats(tokens,handle) :\n",
    "    \"\"\"\n",
    "        Given a list of tokens, print number of tokens, number of unique tokens, \n",
    "        number of characters, lexical diversity (https://en.wikipedia.org/wiki/Lexical_diversity), \n",
    "        and num_tokens most common tokens. Return a list with the number of tokens, number\n",
    "        of unique tokens, lexical diversity, and number of characters. \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    num_tokens = len(tokens)\n",
    "    num_unique_tokens = len(set(tokens))\n",
    "    lexical_diversity = ld.ttr(tokens)\n",
    "    num_characters = sum([len(token) for token in tokens])\n",
    "    top5_tokens = Counter(tokens).most_common()[:5]\n",
    "    \n",
    "    print(\"For\", handle, \": \\n\")\n",
    "    print(f\"There are {num_tokens} tokens in the data.\")\n",
    "    print(f\"There are {num_unique_tokens} unique tokens in the data.\")\n",
    "    print(f\"The lexical diversity is {lexical_diversity:.3f} in the data.\")\n",
    "    print(f\"There are {num_characters} characters in the data.\")\n",
    "    print(f\"These are the five most common tokens in the data: {top5_tokens}.\")\n",
    "\n",
    "    return \"\\n\\n\"\n",
    "\n",
    "# text cleaning functions\n",
    "def remove_url(s): \n",
    "    return re.sub(r'http\\S+', '', s)\n",
    "\n",
    "def lower_case(text):\n",
    "    return text.casefold()\n",
    " \n",
    "def remove_punctuation(text, punct_set=tw_punct) : \n",
    "    return(\"\".join([ch for ch in text if ch not in punct_set]))\n",
    "\n",
    "def tokenize(text) : \n",
    "    \"\"\" Splitting on whitespace rather than the book's tokenize function. That \n",
    "        function will drop tokens like '#hashtag' or '2A', which we need for Twitter. \"\"\"\n",
    "    return text.split()\n",
    "\n",
    "def remove_stop(tokens) :\n",
    "    # modify this function to remove stopwords\n",
    "    tokens = [i for i in tokens if not i in sw]\n",
    "    return tokens\n",
    "\n",
    "def remove_emoji(tokens) :\n",
    "    dem = demoji.findall(tokens)\n",
    "    for item in dem.keys():\n",
    "        tokens = tokens.replace(item, ' ')\n",
    "    return tokens\n",
    "\n",
    "# pipeline implementation\n",
    "def prepare(text, pipeline) : \n",
    "    tokens = str(text)\n",
    "    \n",
    "    for transform in pipeline : \n",
    "        tokens = transform(tokens)\n",
    "        \n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e138eb",
   "metadata": {},
   "source": [
    "### Tokenization and Normalization\n",
    "\n",
    "**Cleaning steps** \n",
    "\n",
    "* remove urls\n",
    "* Casefold to lowercase\n",
    "* Remove punctuation other than hashtags\n",
    "* Split on whitespace\n",
    "* Remove stopwords\n",
    "* Remove emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7dea64e4-5307-4c79-9ce7-7856a6d8acd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE FOR EVERYONE ELSE LATER\n",
    "bitcoin = open('/Users/evachow/Documents/GitHub/ADS509/ADS-509_Final_Project_Team1/twitter/Bitcoin_tweets.txt', 'r')\n",
    "cardano = open('/Users/evachow/Documents/GitHub/ADS509/ADS-509_Final_Project_Team1/twitter/Cardano_tweets.txt', 'r')\n",
    "dogecoin = open('/Users/evachow/Documents/GitHub/ADS509/ADS-509_Final_Project_Team1/twitter/dogecoin_tweets.txt', 'r')\n",
    "ethereum = open('/Users/evachow/Documents/GitHub/ADS509/ADS-509_Final_Project_Team1/twitter/ethereum_tweets.txt', 'r')\n",
    "shibainucoin = open('/Users/evachow/Documents/GitHub/ADS509/ADS-509_Final_Project_Team1/twitter/ShibainuCoin_tweets.txt', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58c28b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "crypto_pipeline1 = [remove_url, lower_case, remove_punctuation, tokenize, remove_stop]\n",
    "# crypto_pipeline = [remove_url, lower_case, remove_punctuation, tokenize, remove_stop]\n",
    "tweets_df[\"tokens\"] = tweets_df[\"tweets\"].apply(prepare,pipeline=crypto_pipeline1)\n",
    "tweets_df[\"num_tokens\"] = tweets_df[\"tokens\"].map(len) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "147fa9ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crypto_type</th>\n",
       "      <th>tweets</th>\n",
       "      <th>tokens</th>\n",
       "      <th>num_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bitcoin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  crypto_type tweets tokens  num_tokens\n",
       "0     Bitcoin    NaN  [nan]           1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e138845a",
   "metadata": {},
   "source": [
    "### Calculate Descriptive Statistics On the Cryptos Tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5694239a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Bitcoin : \n",
      "\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "There are 3 characters in the data.\n",
      "These are the five most common tokens in the data: [('nan', 1)].\n",
      "\n",
      "\n",
      "\n",
      "For ethereum : \n",
      "\n",
      "There are 0 tokens in the data.\n",
      "There are 0 unique tokens in the data.\n",
      "The lexical diversity is 0.000 in the data.\n",
      "There are 0 characters in the data.\n",
      "These are the five most common tokens in the data: [].\n",
      "\n",
      "\n",
      "\n",
      "For Cardano : \n",
      "\n",
      "There are 0 tokens in the data.\n",
      "There are 0 unique tokens in the data.\n",
      "The lexical diversity is 0.000 in the data.\n",
      "There are 0 characters in the data.\n",
      "These are the five most common tokens in the data: [].\n",
      "\n",
      "\n",
      "\n",
      "For dogecoin : \n",
      "\n",
      "There are 0 tokens in the data.\n",
      "There are 0 unique tokens in the data.\n",
      "The lexical diversity is 0.000 in the data.\n",
      "There are 0 characters in the data.\n",
      "These are the five most common tokens in the data: [].\n",
      "\n",
      "\n",
      "\n",
      "For ShibainuCoin : \n",
      "\n",
      "There are 0 tokens in the data.\n",
      "There are 0 unique tokens in the data.\n",
      "The lexical diversity is 0.000 in the data.\n",
      "There are 0 characters in the data.\n",
      "These are the five most common tokens in the data: [].\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for handle in handles:\n",
    "    token_sum = []\n",
    "    start_index = tweets_df['crypto_type'].ne(handle).idxmin()\n",
    "    end_index = len(tweets_df.loc[tweets_df['crypto_type'] == handle])\n",
    "    for i in range(start_index, start_index+end_index):\n",
    "        token_row = tweets_df['tokens'].loc[tweets_df['crypto_type'] == handle][i] \n",
    "        token_sum = token_sum + token_row # use + to stitch all list together\n",
    "    print(descriptive_stats(token_sum, handle))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5768d935",
   "metadata": {},
   "source": [
    "### Build Word Clouds For All The Cryptos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07ae8cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(df, column='tokens', preprocess=None, min_freq=2):\n",
    "\n",
    "    # process tokens and update counter\n",
    "    def update(doc):\n",
    "        tokens = doc if preprocess is None else preprocess(doc)\n",
    "        counter.update(tokens)\n",
    "\n",
    "    # create counter and run through all data\n",
    "    counter = Counter()\n",
    "    df[column].map(update)\n",
    "\n",
    "    # transform counter into data frame\n",
    "    freq_df = pd.DataFrame.from_dict(counter, orient='index', columns=['freq'])\n",
    "    freq_df = freq_df.query('freq >= @min_freq')\n",
    "    freq_df.index.name = 'token'\n",
    "    \n",
    "    return freq_df.sort_values('freq', ascending=False)\n",
    "\n",
    "def wordcloud(word_freq, title=None, max_words=200, stopwords=None):\n",
    "\n",
    "    wc = WordCloud(width=800, height=400, \n",
    "                   background_color= \"black\", colormap=\"Paired\", \n",
    "                   max_font_size=150, max_words=max_words)\n",
    "    \n",
    "    # convert data frame into dict\n",
    "    if type(word_freq) == pd.Series:\n",
    "        counter = Counter(word_freq.fillna(0).to_dict())\n",
    "    else:\n",
    "        counter = word_freq\n",
    "\n",
    "    # filter stop words in frequency counter\n",
    "    if sw is not None:\n",
    "        counter = {token:freq for (token, freq) in counter.items() \n",
    "                              if token not in sw}\n",
    "    wc.generate_from_frequencies(counter)\n",
    " \n",
    "    plt.title(title) \n",
    "\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "552cf905",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "We need at least 1 word to plot a word cloud, got 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vc/ts8zgw4j4sbf_4rwblbwxrf40000gn/T/ipykernel_16468/759038510.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mwc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtweets_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'crypto_type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Bitcoin'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwordcloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwc1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'freq'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/vc/ts8zgw4j4sbf_4rwblbwxrf40000gn/T/ipykernel_16468/334950013.py\u001b[0m in \u001b[0;36mwordcloud\u001b[0;34m(word_freq, title, max_words, stopwords)\u001b[0m\n\u001b[1;32m     33\u001b[0m         counter = {token:freq for (token, freq) in counter.items() \n\u001b[1;32m     34\u001b[0m                               if token not in sw}\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mwc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_frequencies\u001b[0;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0mfrequencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrequencies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mitemgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrequencies\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m             raise ValueError(\"We need at least 1 word to plot a word cloud, \"\n\u001b[0m\u001b[1;32m    411\u001b[0m                              \"got %d.\" % len(frequencies))\n\u001b[1;32m    412\u001b[0m         \u001b[0mfrequencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrequencies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: We need at least 1 word to plot a word cloud, got 0."
     ]
    }
   ],
   "source": [
    "wc1 = count_words(tweets_df.loc[tweets_df['crypto_type'] == 'Bitcoin'], column='tokens', preprocess=None, min_freq=2)\n",
    "wordcloud(wc1['freq'], title=None, max_words=200, stopwords=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f2cd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc2 = count_words(tweets_df.loc[tweets_df['crypto_type'] == 'ethereum'], column='tokens', preprocess=None, min_freq=2)\n",
    "wordcloud(wc2['freq'], title=None, max_words=200, stopwords=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade101ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc3 = count_words(tweets_df.loc[tweets_df['crypto_type'] == 'Cardano'], column='tokens', preprocess=None, min_freq=2)\n",
    "wordcloud(wc3['freq'], title=None, max_words=200, stopwords=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8d74f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc4 = count_words(tweets_df.loc[tweets_df['crypto_type'] == 'dogecoin'], column='tokens', preprocess=None, min_freq=2)\n",
    "wordcloud(wc4['freq'], title=None, max_words=200, stopwords=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8a80b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc5 = count_words(tweets_df.loc[tweets_df['crypto_type'] == 'ShibainuCoin'], column='tokens', preprocess=None, min_freq=2)\n",
    "wordcloud(wc5['freq'], title=None, max_words=200, stopwords=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5282885f",
   "metadata": {},
   "source": [
    "### Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b97074c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = []\n",
    "for i in tweets_df['tokens']:\n",
    "    all_tokens += i\n",
    "\n",
    "# look at some random sampling tokens\n",
    "print(random.sample(all_tokens, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81aacfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cutoff = 5\n",
    "word_dist = nltk.FreqDist(all_tokens)\n",
    "feature_words = set()\n",
    "\n",
    "for word, count in word_dist.items() :\n",
    "    if count > word_cutoff :\n",
    "        feature_words.add(word)\n",
    "        \n",
    "print(f\"With a word cutoff of {word_cutoff}, we have {len(feature_words)} as features in the model (including emojis).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdfaf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at some random sampling feature words\n",
    "print(random.sample(feature_words, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c703eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_features(text,fw) :\n",
    "    \n",
    "    ret_word = [i for i in text]\n",
    "    ret_bool = [True if i in fw else False for i in text]\n",
    "    \n",
    "    ret_dict = dict(zip(ret_word,ret_bool))\n",
    "    \n",
    "    return(ret_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7356b904",
   "metadata": {},
   "outputs": [],
   "source": [
    "convention_data = [[tweets_df['tokens'][i], tweets_df['crypto_type'][i]] for i in range(len(tweets_df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0ba654",
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(conv_features(tokens,feature_words), crypto) for (tokens, crypto) in convention_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb7ed26",
   "metadata": {},
   "source": [
    "Each of the tweet contains a feature set, let's print some feature sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818358d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(featuresets[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5903bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1314)\n",
    "random.shuffle(featuresets)\n",
    "\n",
    "# choose train:test = 7:3 \n",
    "test_size = round(len(featuresets)*(3/10))\n",
    "test_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eb6ff1",
   "metadata": {},
   "source": [
    "#### Naive Bayes\n",
    "##### 1. With emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8979d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set, train_set = featuresets[:test_size], featuresets[test_size:]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print('The training set accuracy for Naive Bayes model with emojis is:',round(nltk.classify.accuracy(classifier, train_set),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fc1e27",
   "metadata": {},
   "source": [
    "Let's look at the top 25 most informative features/tokens of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57208c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.show_most_informative_features(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1783d0ce",
   "metadata": {},
   "source": [
    "For the 25 most informative features, 6 of them are emojis and top5 features have 3 emojis!\n",
    "\n",
    "All of them emoji features are pointing out that for Shiba Inu's tweets are symbolic for its use of emojis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ac9329",
   "metadata": {},
   "source": [
    "Let's run some prediction on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b89176",
   "metadata": {},
   "outputs": [],
   "source": [
    "cryptos = handles\n",
    "results = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "random.shuffle(test_set)\n",
    "\n",
    "for idx, tp in enumerate(test_set) :\n",
    "    tokens, crypto = tp    \n",
    "    # Now do the same thing as above, but we store the results rather\n",
    "    # than printing them. \n",
    "   \n",
    "    # get the estimated party\n",
    "    estimated_crypto = classifier.classify(conv_features(tokens,feature_words))\n",
    "    \n",
    "    results[crypto][estimated_crypto] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e90de64",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84c4b6a",
   "metadata": {},
   "source": [
    "Let's look at some confusion matrix for better visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee124828",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = [test_set[i][1] for i in range(len(test_set))]\n",
    "predicted = [classifier.classify(conv_features(test_set[i][0],feature_words)) for i in range(len(test_set))] \n",
    "\n",
    "cm = confusion_matrix(actual, predicted)\n",
    "\n",
    "# Creating a dataframe for a array-formatted Confusion matrix,so it will be easy for plotting.\n",
    "cm_df = pd.DataFrame(cm, index = handles, columns = handles)\n",
    "\n",
    "#Plotting the confusion matrix\n",
    "font = {'weight' : 'normal',\n",
    "        'size'   : 14}\n",
    "\n",
    "plt.rc('font', **font)\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(cm_df, annot=True,fmt='g')\n",
    "plt.title('Naive Bayes Confusion Matrix')\n",
    "plt.ylabel('Actal Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.show()\n",
    "\n",
    "def compute_accuracy(y_true, y_pred):\n",
    "    correct_predictions = 0\n",
    "    # iterate over each label and check\n",
    "    for true, predicted in zip(y_true, y_pred):\n",
    "        if true == predicted:\n",
    "            correct_predictions += 1\n",
    "    # compute the accuracy\n",
    "    accuracy = correct_predictions/len(y_true)\n",
    "    return accuracy\n",
    "print('\\n')\n",
    "print(\"The test set accuracy of Naive Bayes model with emojis:\", round(compute_accuracy(actual, predicted),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d76294",
   "metadata": {},
   "source": [
    "Accuracy of 0.959 for training and 0.829 for test is very good for our model performance.\n",
    "\n",
    "We suspect that the emojis are playing a significant role in the model performance as they can be unique to certain type of coins.\n",
    "\n",
    "We will now remove the emojis and see if our hypothesis is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152e0117",
   "metadata": {},
   "source": [
    "##### 2. Without emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4beb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a different pipeline including remove_emoji \n",
    "crypto_pipeline2 = [remove_url, lower_case, remove_punctuation, remove_emoji, tokenize, remove_stop]\n",
    "\n",
    "# add a new column for tokens without emojis\n",
    "tweets_df[\"tokens_no_emoji\"] = tweets_df[\"tweets\"].apply(prepare,pipeline=crypto_pipeline2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0e3d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens_no_emoji = []\n",
    "for i in tweets_df['tokens_no_emoji']:\n",
    "    all_tokens_no_emoji += i\n",
    "\n",
    "# look at some random sampling tokens\n",
    "print(random.sample(all_tokens_no_emoji, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb7cf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cutoff = 5\n",
    "word_dist = nltk.FreqDist(all_tokens_no_emoji)\n",
    "feature_words_no_emoji = set()\n",
    "\n",
    "for word, count in word_dist.items() :\n",
    "    if count > word_cutoff :\n",
    "        feature_words_no_emoji.add(word)\n",
    "        \n",
    "print(f\"With a word cutoff of {word_cutoff}, we have {len(feature_words_no_emoji)} as features in the model (without emojis).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f7f2bf",
   "metadata": {},
   "source": [
    "Compared to 1812 features with emojis included, we removed 89 emojis from the feature sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f5d8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "convention_data_no_emoji = [[tweets_df['tokens_no_emoji'][i], tweets_df['crypto_type'][i]] for i in range(len(tweets_df))]\n",
    "featuresets_no_emoji = [(conv_features(tokens,feature_words_no_emoji), crypto) for (tokens, crypto) in convention_data_no_emoji]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761a9968",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1314)\n",
    "random.shuffle(featuresets)\n",
    "\n",
    "# choose train:test = 7:3 \n",
    "test_size_no_emoji = round(len(featuresets_no_emoji)*(3/10))\n",
    "test_size_no_emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e96833",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_no_emoji, train_set_no_emoji = featuresets_no_emoji[:test_size_no_emoji], featuresets_no_emoji[test_size_no_emoji:]\n",
    "classifier_no_emoji = nltk.NaiveBayesClassifier.train(train_set_no_emoji)\n",
    "print('The training set accuracy for Naive Bayes model without emojis is:\"',round(nltk.classify.accuracy(classifier_no_emoji, train_set),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6207ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_no_emoji.show_most_informative_features(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844b1b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cryptos = handles\n",
    "results = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "random.shuffle(test_set)\n",
    "\n",
    "for idx, tp in enumerate(test_set) :\n",
    "    tokens, crypto = tp    \n",
    "    # Now do the same thing as above, but we store the results rather\n",
    "    # than printing them. \n",
    "   \n",
    "    # get the estimated party\n",
    "    estimated_crypto_no_emoji = classifier_no_emoji.classify(conv_features(tokens,feature_words_no_emoji))\n",
    "    \n",
    "    results[crypto][estimated_crypto_no_emoji] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088c5896",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6f7805",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_no_emoji = [test_set_no_emoji[i][1] for i in range(len(test_set_no_emoji))]\n",
    "predicted_no_emoji = [classifier_no_emoji.classify(conv_features(test_set[i][0],feature_words_no_emoji)) for i in range(len(test_set_no_emoji))] \n",
    "\n",
    "cm_no_emoji = confusion_matrix(actual_no_emoji, predicted_no_emoji)\n",
    "\n",
    "# Creating a dataframe for a array-formatted Confusion matrix,so it will be easy for plotting.\n",
    "cm_df_no_emoji = pd.DataFrame(cm_no_emoji, index = handles, columns = handles)\n",
    "\n",
    "#Plotting the confusion matrix\n",
    "font = {'weight' : 'normal',\n",
    "        'size'   : 14}\n",
    "\n",
    "plt.rc('font', **font)\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(cm_df_no_emoji, annot=True,fmt='g')\n",
    "plt.title('Naive Bayes Confusion Matrix')\n",
    "plt.ylabel('Actal Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.show()\n",
    "\n",
    "print('\\n')\n",
    "print(\"The test set accuracy of Naive Bayes model without emojis:\", round(compute_accuracy(actual_no_emoji, predicted_no_emoji),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98d9954",
   "metadata": {},
   "source": [
    "WOW! With an accuracy score on training set of 0.767 and less than 0.1 on test set of mere 0.072, our model could barely recognize anything cryptocurrency using words alone without any emojis!\n",
    "\n",
    "This results confirms that our hypothesis is proven to be correct and it is a learnt fact that emojis are indeed a key factor for classifying different cryptocurrencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecfcf01",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e443cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_svm = nltk.classify.SklearnClassifier(LinearSVC())\n",
    "classifier_svm.train(train_set)\n",
    "print('The training set accuracy for SVM model is:',round(nltk.classify.accuracy(classifier_svm, train_set),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd62a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "cryptos = handles\n",
    "results = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "random.shuffle(test_set)\n",
    "\n",
    "for idx, tp in enumerate(test_set) :\n",
    "    tokens, crypto = tp    \n",
    "    # Now do the same thing as above, but we store the results rather\n",
    "    # than printing them. \n",
    "   \n",
    "    # get the estimated party\n",
    "    estimated_crypto = classifier_svm.classify(conv_features(tokens,feature_words))\n",
    "    \n",
    "    results[crypto][estimated_crypto] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82be3d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e416e7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_svm = [test_set[i][1] for i in range(len(test_set))]\n",
    "predicted_svm = [classifier_svm.classify(conv_features(test_set[i][0],feature_words)) for i in range(len(test_set))] \n",
    "\n",
    "cm_svm = confusion_matrix(actual_svm, predicted_svm)\n",
    "\n",
    "# Creating a dataframe for a array-formatted Confusion matrix,so it will be easy for plotting.\n",
    "cm_svm_df = pd.DataFrame(cm_svm, index = handles, columns = handles)\n",
    "\n",
    "#Plotting the confusion matrix\n",
    "font = {'weight' : 'normal',\n",
    "        'size'   : 14}\n",
    "\n",
    "plt.rc('font', **font)\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(cm_svm_df, annot=True,fmt='g')\n",
    "plt.title('SVM Confusion Matrix')\n",
    "plt.ylabel('Actal Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.show()\n",
    "\n",
    "print('\\n')\n",
    "print(\"The test set accuracy of SVM model is:\", round(compute_accuracy(actual_svm, predicted_svm),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756e30bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_compare = pd.DataFrame({\n",
    "                            'Models': ['Naive Bayes', 'SVM'],\n",
    "                            'Training Accuracy': [round(nltk.classify.accuracy(classifier, train_set),3), round(nltk.classify.accuracy(classifier_svm, train_set),3)],\n",
    "                            'Test Accuracy': [round(compute_accuracy(actual, predicted),3), round(compute_accuracy(actual_svm, predicted_svm),3)]\n",
    "                             })\n",
    "class_compare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0003adf",
   "metadata": {},
   "source": [
    "Both models have been performing with acceptable result and SVM outperforms Naive Bayes slightly with 0.981 of training accuracy and 0.853 test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520201bd",
   "metadata": {},
   "source": [
    "### Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef0c37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d062f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add some helpful columns on the df\n",
    "tweets_df['char_len'] = tweets_df['tweets'].apply(len)\n",
    "tweets_df['word_len'] = tweets_df['tweets'].apply(lambda x: len(x.split()))\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666f7d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# checking to see which crypto has the most words\n",
    "tweets_df.groupby('crypto_type').agg({'word_len': 'mean'}).plot.bar(figsize=(10,6), rot = 0,ylabel = 'Counts',title = 'Average Tweet Length Comparison')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baafdd0a",
   "metadata": {},
   "source": [
    "Shiba Inu has the longest average tweet length while Bitcoin has surprisingly the shortest!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02924ccd",
   "metadata": {},
   "source": [
    "#### Count and TF-IDF vectorizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26ce4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a cutoff of minimum of 5 words and 70% as maximum.\n",
    "count_tweet_vectorizer = CountVectorizer(stop_words=sw, min_df=5, max_df=0.7)\n",
    "count_tweet_vectors = count_tweet_vectorizer.fit_transform(tweets_df[\"tweets\"])\n",
    "count_tweet_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a067e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(count_tweet_vectors.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539b6ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_tweet_vectorizer = TfidfVectorizer(stop_words=sw, min_df=5, max_df=0.7)\n",
    "tfidf_tweet_vectors = tfidf_tweet_vectorizer.fit_transform(tweets_df['tweets'])\n",
    "tfidf_tweet_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab72024",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(tfidf_tweet_vectors.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96895e5e",
   "metadata": {},
   "source": [
    "The `count_tweet_vectors` holds the absolute counts for each word in different corpura while\n",
    "the TfidfTransformer (`tfidf_tweet_vectors`) transforms this count matrix to a \n",
    "normalized tf or tf-idf representation in float format (percentage).\n",
    "\n",
    "The two topic-document matrice can help understand the dimensionality of the keywords and documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1408748c",
   "metadata": {},
   "source": [
    "#### Display Topics Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded42522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, features, no_top_words=5):\n",
    "    for topic, words in enumerate(model.components_):\n",
    "        total = words.sum()\n",
    "        largest = words.argsort()[::-1] # invert sort order\n",
    "        print(\"\\nTopic %02d\" % topic)\n",
    "        for i in range(0, no_top_words):\n",
    "            print(\" %s (%2.2f)\" % (features[largest[i]], abs(words[largest[i]]*100.0/total)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3b7f8a",
   "metadata": {},
   "source": [
    "#### Non-Negative Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09f6551",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_tweet_model = NMF(n_components=5, random_state=314)\n",
    "W_tweet_matrix = nmf_tweet_model.fit_transform(tfidf_tweet_vectors)\n",
    "H_tweet_matrix = nmf_tweet_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cc1d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(nmf_tweet_model, tfidf_tweet_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f40cd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_W_tweet_matrix = pd.DataFrame(W_tweet_matrix)\n",
    "nmf_W_tweet_matrix.columns = ['Topic 00', 'Topic 01', 'Topic 02', 'Topic 03', 'Topic 04']\n",
    "nmf_W_tweet_matrix['Topic'] = nmf_W_tweet_matrix.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff626af",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_topic = pd.concat([nmf_W_tweet_matrix[['Topic']], tweets_df[['crypto_type']]], axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e38846",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_topic.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a9c4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_topic.groupby('Topic')['crypto_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed8223c",
   "metadata": {},
   "source": [
    "#### Latent Semantic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1806525",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_tweet_model = TruncatedSVD(n_components=5, n_iter=7, random_state=314)\n",
    "svd_W_tweet_matrix = svd_tweet_model.fit_transform(tfidf_tweet_vectors)\n",
    "svd_H_tweet_matrix = svd_tweet_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee37bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(svd_tweet_model, tfidf_tweet_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38547080",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_W_tweet_matrix = pd.DataFrame(svd_W_tweet_matrix)\n",
    "svd_W_tweet_matrix.columns = ['Topic 00', 'Topic 01', 'Topic 02', 'Topic 03', 'Topic 04']\n",
    "svd_W_tweet_matrix['Topic'] = svd_W_tweet_matrix.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47efa5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_topic = pd.concat([svd_W_tweet_matrix[['Topic']], tweets_df[['crypto_type']]], axis=1 )\n",
    "svd_topic.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810c99f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_topic.groupby('Topic')['crypto_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deada6a7",
   "metadata": {},
   "source": [
    "#### Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a3a373",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_tweet_model = LatentDirichletAllocation(n_components=5, random_state=314)\n",
    "lda_W_tweet_matrix = lda_tweet_model.fit_transform(count_tweet_vectors)\n",
    "lda_H_tweet_matrix = lda_tweet_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dd13ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(lda_tweet_model, tfidf_tweet_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f81b295",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_W_tweet_matrix = pd.DataFrame(lda_W_tweet_matrix)\n",
    "lda_W_tweet_matrix.columns = ['Topic 00', 'Topic 01', 'Topic 02', 'Topic 03', 'Topic 04']\n",
    "lda_W_tweet_matrix['Topic'] = lda_W_tweet_matrix.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b329844e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_topic = pd.concat([lda_W_tweet_matrix[['Topic']], tweets_df[['crypto_type']]], axis=1 )\n",
    "lda_topic.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad47f371",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_topic.groupby('Topic')['crypto_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba84c81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_display = pyLDAvis.sklearn.prepare(lda_tweet_model, count_tweet_vectors, count_tweet_vectorizer, sort_topics=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4916930a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.display(lda_display)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
